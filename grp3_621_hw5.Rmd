---
title: 'Data 621 Hw #5'
author: 'Group #3'
date: "2024-04-18"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Wine Data

Goal: to Build a count regression model to predict the number of cases that will be sold given certain properties of wine.

### Packages

```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(summarytools)
library(corrplot)
library(gt)
library(pROC)
library(MASS)
library(glue)
```



### Importing Data

```{r}
wine_train_raw=read.csv(url("https://raw.githubusercontent.com/sleepysloth12/data621_hw5/main/wine-training-data.csv"))

wine_test_raw=read.csv(url("https://raw.githubusercontent.com/sleepysloth12/data621_hw5/main/wine-evaluation-data.csv"))

```


## Part I

#WILL COME BACK AND ADD MORE WORDS (Marjete)

```{r}
summary_stats <- wine_train_raw %>%
  summarise(
    Mean_Target = mean(TARGET, na.rm = TRUE),
    SD_Target = sd(TARGET, na.rm = TRUE),
    Median_Target = median(TARGET, na.rm = TRUE),
    Mean_AcidIndex = mean(AcidIndex, na.rm = TRUE),
    SD_AcidIndex = sd(AcidIndex, na.rm = TRUE),
    Median_AcidIndex = median(AcidIndex, na.rm = TRUE),
    Mean_Alcohol = mean(Alcohol, na.rm = TRUE),
    SD_Alcohol = sd(Alcohol, na.rm = TRUE),
    Median_Alcohol = median(Alcohol, na.rm = TRUE),
    Mean_Chlorides = mean(Chlorides, na.rm = TRUE),
    SD_Chlorides = sd(Chlorides, na.rm = TRUE),
    Median_Chlorides = median(Chlorides, na.rm = TRUE),
    Mean_CitricAcid = mean(CitricAcid, na.rm = TRUE),
    SD_CitricAcid = sd(CitricAcid, na.rm = TRUE),
    Median_CitricAcid = median(CitricAcid, na.rm = TRUE),
    Mean_Density = mean(Density, na.rm = TRUE),
    SD_Density = sd(Density, na.rm = TRUE),
    Median_Density = median(Density, na.rm = TRUE),
    Mean_FixedAcidity = mean(FixedAcidity, na.rm = TRUE),
    SD_FixedAcidity = sd(FixedAcidity, na.rm = TRUE),
    Median_FixedAcidity = median(FixedAcidity, na.rm = TRUE),
    Mean_FreeSulfurDioxide = mean(FreeSulfurDioxide, na.rm = TRUE),
    SD_FreeSulfurDioxide = sd(FreeSulfurDioxide, na.rm = TRUE),
    Median_FreeSulfurDioxide = median(FreeSulfurDioxide, na.rm = TRUE),
    Mean_LabelAppeal = mean(LabelAppeal, na.rm = TRUE),
    SD_LabelAppeal = sd(LabelAppeal, na.rm = TRUE),
    Median_LabelAppeal = median(LabelAppeal, na.rm = TRUE),
    Mean_ResidualSugar = mean(ResidualSugar, na.rm = TRUE),
    SD_ResidualSugar = sd(ResidualSugar, na.rm = TRUE),
    Median_ResidualSugar = median(ResidualSugar, na.rm = TRUE),
    Mean_STARS = mean(STARS, na.rm = TRUE),
    SD_STARS = sd(STARS, na.rm = TRUE),
    Median_STARS = median(STARS, na.rm = TRUE),
    Mean_Sulphates = mean(Sulphates, na.rm = TRUE),
    SD_Sulphates = sd(Sulphates, na.rm = TRUE),
    Median_Sulphates = median(Sulphates, na.rm = TRUE),
    Mean_TotalSulfurDioxide = mean(TotalSulfurDioxide, na.rm = TRUE),
    SD_TotalSulfurDioxide = sd(TotalSulfurDioxide, na.rm = TRUE),
    Median_TotalSulfurDioxide = median(TotalSulfurDioxide, na.rm = TRUE),
    Mean_VolatileAcidity = mean(VolatileAcidity, na.rm = TRUE),
    SD_VolatileAcidity = sd(VolatileAcidity, na.rm = TRUE),
    Median_VolatileAcidity = median(VolatileAcidity, na.rm = TRUE),
    Mean_pH = mean(pH, na.rm = TRUE),
    SD_pH = sd(pH, na.rm = TRUE),
    Median_pH = median(pH, na.rm = TRUE)
  ) %>%
  pivot_longer(everything(), names_to = "Statistic", values_to = "Value") %>%
  separate(Statistic, into = c("Measure", "Variable"), sep = "_") %>%
  pivot_wider(names_from = Measure, values_from = Value) %>%
  dplyr::select(Variable, Mean, SD, Median) %>%
  mutate(Variable = case_when(
    Variable == "TARGET" ~ "Target",
    Variable == "AcidIndex" ~ "Acid Index",
    Variable == "Alcohol" ~ "Alcohol Content",
    Variable == "Chloride" ~ "Chloride Concentration",
    Variable == "CitricAcid" ~ "Citric Acid Content",
    Variable == "Density" ~ "Density",
    Variable == "FixedAcidity" ~ "Fixed Acidity",
    Variable == "FreeSulfurDioxide" ~ "Free Sulfur Dioxide",
    Variable == "LabelAppeal" ~ "Label Appeal",
    Variable == "ResidualSugar" ~ "Residual Sugar",
    Variable == "STARS" ~ "STARS",
    Variable == "Sulphates" ~ "Sulphates Content",
    Variable == "TotalSulfurDioxide" ~ "Total Sulfur Dioxide", 
    Variable == "VolatileAcidity" ~ "Volatile Acidity",
    Variable == "pH" ~ "pH",
    TRUE ~ Variable
  ))

summary_stats %>%
  gt() %>%
  tab_header(
    title = "Summary Statistics of Predictor Variables"
  ) %>%
  cols_label(
    Variable = "Variable",
    Mean = "Mean",
    SD = "Standard Deviation",
    Median = "Median"
  )

```
```{r}
train_dat_long = wine_train_raw %>%
  dplyr::select(-INDEX) %>%  
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value")

train_dat_long <- train_dat_long[complete.cases(train_dat_long$Value), ]

options(repr.plot.width=20, repr.plot.height=20) 
#why are graphs not changing in size 
ggplot(train_dat_long, aes(x = Value)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) + 
  theme_minimal() +
  labs(title = "Distribution of Predictor Variables", x = "Value", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
#box plot not including the index column
ggplot(train_dat_long, aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "blue", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Box Plot of Predictor Variables", x = "Variable", y = "Value") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        text = element_text(size = 12)) # Adjust font size for better readability
```


There is missing data in the following columns: ResidualSugar, Chlorides, FreeSulfurDioxide, TotalSulfurDioxide, pH, Sulphates, Alcohol, STARS
```{r}
colSums(is.na(wine_train_raw))
```


### Summary Statistics and Distribution

```{r}
summary1 = dfSummary(wine_train_raw[, -which(names(wine_train_raw) == "INDEX")])


print(summary1, method = "render")

view(summary1)
```


### Correlation Matrix

```{r}
numeric_data = wine_train_raw %>% 
  select(-TARGET) %>% 
  select_if(is.numeric)  

correlation_matrix = cor(numeric_data, use = "complete.obs")  

print(correlation_matrix)



corrplot(correlation_matrix, method = "color", type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, addCoef.col = "black")
```

Weak Correlations Mostly: 

Most variables exhibit very weak correlations with each other, which are close to 0. This suggests no strong linear relationships between most pairs of features.

LabelAppeal and STARS: The strongest correlation observed is between LabelAppeal and STARS (0.3189). This suggests a moderate positive relationship, where wines with more appealing labels tend to be rated higher by experts.


FixedAcidity and AcidIndex: There's a noticeable positive correlation between FixedAcidity and AcidIndex (0.1542). Although not very strong, it indicates that as fixed acidity increases, the overall acidity index tends to increase as well.


Alcohol and STARS: Alcohol content shows a modest correlation with STARS (0.0649), suggesting that higher alcohol content could be slightly associated with higher ratings by experts.


Negative Correlations: There are several mild negative correlations, such as AcidIndex and STARS (-0.0955), indicating that higher acid index values may slightly correlate with lower expert ratings.

## Part II

### Looking For Missing Values and Fixing it 

The following are missing:

616 Records of `ResidualSugar` (4.8%)
638 Records of `Chlorides` (5%)
647 Records of `FreeSulfurDioxide` (5.1%)
682 Records of `TotalSulfurDioxide` (5.3%)
395 Records of `pH` (3.1%)
1210 Records of `Sulphates` (9.5%)
653 Records of `Alcohol` (5.1%)
3359 Records of `STARS` (26.3%)

All of these columns (with the exception of `STARS`) are normally distributed. 

To fill in the missing values, I will use mean and standard deviation by using the `rnorm()` function and creating a function that will process these columns to fill in the missing data.

```{r}
fill_missing_with_normal = function(data, column_name) {
 
  column_data = data[[column_name]]
  
  mean_val = mean(column_data, na.rm = TRUE)
  sd_val = sd(column_data, na.rm = TRUE)
  
  num_missing = sum(is.na(column_data))
  
  new_values = rnorm(num_missing, mean = mean_val, sd = sd_val)
  
  data[[column_name]][is.na(column_data)] = new_values
  
  return(data)
}

columns_with_missing = c("ResidualSugar", "Chlorides", "FreeSulfurDioxide",
                          "TotalSulfurDioxide", "pH", "Sulphates", "Alcohol")

for (column in columns_with_missing) {
  wine_train_raw = fill_missing_with_normal(wine_train_raw, column)
}

for (column in columns_with_missing) {
  wine_test_raw = fill_missing_with_normal(wine_test_raw, column)
}


summary2 = dfSummary(wine_train_raw[, -which(names(wine_train_raw) == "INDEX")])


print(summary2, method = "render")

view(summary2)
```

Now, there are no more missing values for the normally distributed columns.

Only 3359 (26.3%) of records now are missing `STARS` label.

I will use the existing probabilities of the distribution of `STARS` data to do bootstraping to generate the missing values.

```{r}

fill_missing_categorical = function(data, column_name, levels, probabilities) {
  
  column_data = data[[column_name]]
  
 
  num_missing = sum(is.na(column_data))
  
  
  new_values = sample(levels, num_missing, replace = TRUE, prob = probabilities)
  
  
  data[[column_name]][is.na(column_data)] <- new_values
  
  return(data)
}


stars_levels = c(1, 2, 3, 4)


stars_probabilities = c(32.2, 37.8, 23.4, 6.5) / 100


wine_train_raw = fill_missing_categorical(wine_train_raw, "STARS", stars_levels, stars_probabilities)
wine_test_raw = fill_missing_categorical(wine_test_raw, "STARS", stars_levels, stars_probabilities)


summary3 = dfSummary(wine_train_raw[, -which(names(wine_train_raw) == "INDEX")])


print(summary3, method = "render")

view(summary3)

```

No missing values, and we keep the distribution patterns that originally existed. Nice!


### Transforming the Data

Adding interaction terms can help to capture the combined effects of variables.

Acidity Interactions: We will explore interactions between `FixedAcidity`, `VolatileAcidity`, and `CitricAcid`.

`Sulfur Dioxide` and `Free Sulfur Dioxide`: These could interact with other chemical properties like `pH` and `Alcohol` content, affecting wine stability and taste.

Sulfur Ratio: The ratio of `FreeSulfurDioxide` to `TotalSulfurDioxide` might give insights into how bound versus free sulfur dioxide impacts wine quality.

Sugar to Acid Ratio: We will use `ResidualSugar ` to `TotalAcidity` ( `TotalAcidity` as a sum of `FixedAcidity` and other acid measures).

```{r}

wine_train_clean = wine_train_raw %>%
  mutate(
    
    FixedAcid_VolatileAcid = FixedAcidity * VolatileAcidity,
    Alcohol_CitricAcid = Alcohol * CitricAcid,
    Alcohol_pH = Alcohol * pH,
    
    
    SugarToAcidRatio = ResidualSugar / (FixedAcidity + CitricAcid), 
    FreeToTotalSulfur = FreeSulfurDioxide / TotalSulfurDioxide,
    
    
    Acid_Alcohol_Index = (FixedAcidity * VolatileAcidity) / Alcohol
  )


wine_train_clean = wine_train_clean %>%
  dplyr::select(TARGET,  
         FixedAcid_VolatileAcid,  
         Alcohol_CitricAcid,     
         Alcohol_pH,             
         SugarToAcidRatio,        
         FreeToTotalSulfur,       
         Acid_Alcohol_Index,      
         AcidIndex, Alcohol, Chlorides, CitricAcid, Density,
         FixedAcidity, FreeSulfurDioxide, LabelAppeal, pH,
         ResidualSugar, STARS, Sulphates, TotalSulfurDioxide, VolatileAcidity
        )

wine_test_clean = wine_test_raw %>%
  mutate(
    
    FixedAcid_VolatileAcid = FixedAcidity * VolatileAcidity,
    Alcohol_CitricAcid = Alcohol * CitricAcid,
    Alcohol_pH = Alcohol * pH,
    
    
    SugarToAcidRatio = ResidualSugar / (FixedAcidity + CitricAcid), 
    FreeToTotalSulfur = FreeSulfurDioxide / TotalSulfurDioxide,
    
    
    Acid_Alcohol_Index = (FixedAcidity * VolatileAcidity) / Alcohol
  )


wine_test_clean = wine_test_clean %>%
  dplyr::select(TARGET,  
         FixedAcid_VolatileAcid,  
         Alcohol_CitricAcid,     
         Alcohol_pH,             
         SugarToAcidRatio,        
         FreeToTotalSulfur,       
         Acid_Alcohol_Index,      
         AcidIndex, Alcohol, Chlorides, CitricAcid, Density,
         FixedAcidity, FreeSulfurDioxide, LabelAppeal, pH,
         ResidualSugar, STARS, Sulphates, TotalSulfurDioxide, VolatileAcidity
        )


```

### New Correlation Matrix

```{r}
cor_matrix2 = cor(wine_train_clean %>% select(-TARGET), use = "complete.obs")



```

Given the high correlations between the interaction terms and their original variables, I will remove original variables.

```{r}
wine_train_clean = wine_train_clean %>%
  select(TARGET,  
         FixedAcid_VolatileAcid,  
         Alcohol_CitricAcid,     
         Alcohol_pH,             
         SugarToAcidRatio,        
         FreeToTotalSulfur,       
         Acid_Alcohol_Index,      
         AcidIndex,  Chlorides, Density,
          FreeSulfurDioxide, LabelAppeal,
         ResidualSugar, STARS
        )
```

## Part II

### Poisson Regression Model One

```{r}
poisson_1 = glm(TARGET ~ FixedAcid_VolatileAcid + Alcohol_CitricAcid + Alcohol_pH + AcidIndex + Chlorides + Density + FreeSulfurDioxide + LabelAppeal + ResidualSugar + STARS, family = poisson, data = wine_train_clean)

summary(poisson_1)
```

For the first Poisson Regression Model, I selected all variables that would work in a lm() function. The reason why I omitted certain variables was that the lm() would not work with "Inf" values present in the created ratios and indexes.

### Poisson Regression Model Two

```{r}
poisson_2 = glm(TARGET ~ FixedAcid_VolatileAcid + Alcohol_CitricAcid + AcidIndex + Chlorides + Density + FreeSulfurDioxide + LabelAppeal + STARS, family = poisson, data = wine_train_clean)

summary(poisson_2)
```

For the second poisson regression model, I selected all the predictors from the first model which were labeled as significant predictors.

### Negative Binomial Regression Model One

```{r}
nb_1 = MASS::glm.nb(TARGET ~ FixedAcid_VolatileAcid + Alcohol_CitricAcid + Alcohol_pH + AcidIndex + Chlorides + Density + FreeSulfurDioxide + LabelAppeal + ResidualSugar + STARS, data = wine_train_clean)

summary(nb_1)
```

For the first negative binomial regression

### Negative Binomial Regression Model Two

```{r}
nb_2 = MASS::glm.nb(TARGET ~ FixedAcid_VolatileAcid + Alcohol_CitricAcid + AcidIndex + Chlorides + Density + FreeSulfurDioxide + LabelAppeal + STARS, data = wine_train_clean)

summary(nb_2)
```

Similarly, I selected the variables that were labeled as significant predictors.

### Multiple Linear Regression Model One

```{r}
ml_1 = lm(TARGET ~ FixedAcid_VolatileAcid + Alcohol_CitricAcid + Alcohol_pH + AcidIndex + Chlorides + Density + FreeSulfurDioxide + LabelAppeal + ResidualSugar + STARS, data = wine_train_clean) 

summary(ml_1)
```

From the wine_train_clean, I selected all variables that would work in a lm() function. The reason why I omitted certain variables was that the lm() would not work with "Inf" values present in the created ratios and indexes.

### Multiple Linear Regression Model Two

```{r}
ml_2 = step(ml_1)

summary(ml_2)
```

Using the initial multiple linear regression, I applied the step() function to create a more filtered down multiple linear regression model.

# Selecting the Most Appropriate Count Regression Model

## Model Validation

### MSE Calculations

```{r}
predicted_vals_p1 <- predict(poisson_1, type = 'response')
observed_vals_p1 <- wine_train_clean$TARGET
res_p1 <- observed_vals_p1 - predicted_vals_p1
mse_p1 <- mean(res_p1**2)

predicted_vals_p2 <- predict(poisson_2, type = 'response')
observed_vals_p2 <- wine_train_clean$TARGET
res_p2 <- observed_vals_p2 - predicted_vals_p2
mse_p2 <- mean(res_p2**2)

predicted_vals_nb_1 <- predict(nb_1, type = 'response')
observed_vals_nb_1 <- wine_train_clean$TARGET
res_nb_1 <- observed_vals_nb_1 - predicted_vals_nb_1
mse_nb_1 <- mean(res_nb_1**2)

predicted_vals_nb_2 <- predict(nb_2, type = 'response')
observed_vals_nb_2 <- wine_train_clean$TARGET
res_nb_2 <- observed_vals_nb_2 - predicted_vals_nb_2
mse_nb_2 <- mean(res_nb_2**2)

predicted_vals_ml_1 <- predict(ml_1, type = 'response')
observed_vals_ml_1 <- wine_train_clean$TARGET
rse_ml_1 <- observed_vals_ml_1 - predicted_vals_ml_1
mse_ml_1 <- mean(rse_ml_1**2)

predicted_vals_ml_2 <- predict(ml_2, type = 'response')
observed_vals_ml_2 <- wine_train_clean$TARGET
rse_ml_2 <- observed_vals_ml_2 - predicted_vals_ml_2
mse_ml_2 <- mean(rse_ml_2**2)
```

```{r}
cat("Poisson 1 MSE:", mse_p1, "\n")
cat("Poisson 2 MSE:", mse_p2, "\n")
cat("Negative Binomial 1 MSE:", mse_nb_1, "\n")
cat("Negative Binomial 2 MSE:", mse_nb_2, "\n")
cat("Linear Regression 1 MSE:", mse_ml_1, "\n")
cat("Linear Regression 2 MSE:", mse_ml_2, "\n")
```

### Plotted residuals

```{r}
plot(poisson_1)
plot(poisson_2)
plot(nb_1)
plot(nb_2)
plot(ml_1)
plot(ml_2)
```

Based on the analyses performed, it appears that all count models generated exhibit similar performance at face value. Specifically, all models generated yielded similar deviance values, AIC values, and MSE values. That said, the negative binomial models tended to produce slightly better metrics, overall compared to the Poisson models. However, I do not believe a negative binomial model is particularly necessary for our data, as our target variable is likely more aligned with a Poisson or normal distribution. Furthermore, our model is not overdispersed, as evident by the fact that the residual and null deviance are smaller than their respective degrees of freedom. Finally, the maximum number of iterations were reached for both negative binomial models, suggesting difficulties in convergence. For these reasons, I will choose to use a Poisson model for our final evaluation, despite the fact that the negative binomial models present slightly better metrics. Specifically, I will use second poisson model, as the metrics for that model are slightly superior.

I will also mention that I do believe the linear regression models are good fits for our data, as well. As previously mentioned, I believe our target variable follows a rather normal distribution, as a result of the large sample size. In addition, our residual plots do show some violations in normality assumptions (i.e., some non-linearity, and the presence of outliers impacting skewness), but we can see that the assumptions are not overly violated and could likely be rectified via transformations or outlier removal. That said, the current linear models do appear to be poor fits, however, with R^2 values around .25.

### Running Predictions on the Test Dataset

```{r}
TARGET <- predict(poisson_2, newdata = wine_test_clean, type = 'response')

wine_test_clean <- wine_test_clean %>% dplyr::select(-TARGET)

wine_test_clean_scored <- cbind(wine_test_clean, TARGET)

hist(wine_test_clean_scored$TARGET)

```

